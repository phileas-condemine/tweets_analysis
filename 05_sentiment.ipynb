{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate FR to EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.4\n",
      "1.0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(np.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"bonjour je voudrais des chips s'il vous plaÃ®t\",\n",
       " 'serait-il possible de faire moins de bruit ?',\n",
       " 'les burgers sont-ils cuits ?',\n",
       " 'et Ã§a vous fait rire !']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "src_text = [\n",
    "    \"bonjour je voudrais des chips s'il vous plaÃ®t\",\n",
    "    \"serait-il possible de faire moins de bruit ?\",\n",
    "    \"les burgers sont-ils cuits ?\",\n",
    "    \"et Ã§a vous fait rire !\"\n",
    "]\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Hello, I'd like some chips, please.\",\n",
       " 'Would it be possible to make less noise?',\n",
       " 'Are the burgers cooked?',\n",
       " 'And that makes you laugh!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "marianmt_tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "marianmt_model = MarianMTModel.from_pretrained(model_name)\n",
    "translated = marianmt_model.generate(**marianmt_tokenizer(src_text, return_tensors=\"pt\", padding=True))\n",
    "\n",
    "translation = [marianmt_tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict sentiment (for twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Model name 'cardiffnlp/twitter-roberta-base-sentiment' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'cardiffnlp/twitter-roberta-base-sentiment' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8efcfcabc64d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mMODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"cardiffnlp/twitter-roberta-base-{task}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0msentiment_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0msentiment_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \"\"\"\n\u001b[0;32m-> 1428\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   1528\u001b[0m                 \u001b[0;34m\"Model name '{}' was not found in tokenizers model name list ({}). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m                 \u001b[0;34m\"We assumed '{}' was a path, a model identifier, or url to a directory containing vocabulary files \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Model name 'cardiffnlp/twitter-roberta-base-sentiment' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'cardiffnlp/twitter-roberta-base-sentiment' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "sentiment_tokenizer.save_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "sentiment_model.save_pretrained(MODEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_predict_sentiment(txt):\n",
    "    txt = preprocess(txt)\n",
    "    encoded_input = sentiment_tokenizer(txt, return_tensors='pt')\n",
    "    output = sentiment_model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good night ðŸ˜Š'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 59514)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Good night ðŸ˜Š\"\n",
    "\n",
    "preprocess_and_predict_sentiment(text).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[3.39661754e-04, 1.19656454e-08, 1.47925981e-03, ...,\n",
       "         1.35539384e-08, 1.37483847e-08, 8.55769122e-06],\n",
       "        [8.50758166e-04, 4.29434017e-08, 1.79157476e-03, ...,\n",
       "         4.48998705e-08, 4.54324258e-08, 8.55769122e-06],\n",
       "        [2.53506820e-04, 1.77586088e-08, 2.04355828e-03, ...,\n",
       "         2.53600838e-08, 2.65038391e-08, 8.55769122e-06],\n",
       "        ...,\n",
       "        [6.96184361e-05, 1.53040016e-08, 6.98564923e-04, ...,\n",
       "         1.71027015e-08, 1.67914624e-08, 8.55769122e-06],\n",
       "        [8.71811135e-05, 4.79903983e-09, 9.69734334e-04, ...,\n",
       "         5.22298871e-09, 5.20541477e-09, 8.55769122e-06],\n",
       "        [8.13537091e-03, 1.44202525e-08, 2.63848706e-05, ...,\n",
       "         1.51296273e-08, 1.55429003e-08, 8.55769122e-06]], dtype=float32),\n",
       " array([[5.8947215e-03, 4.3190330e-08, 2.3677819e-03, ..., 4.3955236e-08,\n",
       "         4.4659018e-08, 6.3257619e-05],\n",
       "        [4.1102016e-04, 9.1906669e-09, 4.8542814e-05, ..., 9.8096100e-09,\n",
       "         1.0106302e-08, 6.3257619e-05],\n",
       "        [1.9893670e-04, 1.3573186e-08, 6.9121139e-05, ..., 1.4940333e-08,\n",
       "         1.5208906e-08, 6.3257619e-05],\n",
       "        ...,\n",
       "        [1.8174437e-04, 6.7246546e-08, 4.4368443e-04, ..., 6.7988509e-08,\n",
       "         6.8306562e-08, 6.3257619e-05],\n",
       "        [3.7890379e-04, 1.2608145e-08, 1.7338994e-04, ..., 1.3077629e-08,\n",
       "         1.2840839e-08, 6.3257619e-05],\n",
       "        [8.9015149e-02, 1.0131744e-07, 8.3972976e-05, ..., 1.0378681e-07,\n",
       "         1.0647403e-07, 6.3257619e-05]], dtype=float32),\n",
       " array([[3.83764156e-04, 8.01616906e-08, 2.81689805e-03, ...,\n",
       "         9.40076461e-08, 9.51000061e-08, 3.22437954e-05],\n",
       "        [2.37273911e-04, 1.09664434e-07, 1.13542436e-03, ...,\n",
       "         1.18638873e-07, 1.22048647e-07, 3.22437954e-05],\n",
       "        [6.28185575e-04, 2.20524271e-07, 4.35436564e-03, ...,\n",
       "         2.60461348e-07, 2.67563422e-07, 3.22437954e-05],\n",
       "        ...,\n",
       "        [1.07142005e-04, 4.05147844e-08, 1.25599443e-03, ...,\n",
       "         4.49958897e-08, 4.55605118e-08, 3.22437954e-05],\n",
       "        [2.83294532e-04, 1.95446805e-08, 1.29935856e-03, ...,\n",
       "         2.30461801e-08, 2.31368631e-08, 3.22437954e-05],\n",
       "        [6.93093380e-03, 2.51321293e-08, 5.32677805e-04, ...,\n",
       "         2.65830202e-08, 2.68486016e-08, 3.22437954e-05]], dtype=float32),\n",
       " array([[4.9467110e-03, 5.5586892e-08, 2.4793409e-03, ..., 5.5849284e-08,\n",
       "         5.7579715e-08, 4.7664871e-05],\n",
       "        [1.0127129e-03, 1.8588370e-08, 4.2879285e-04, ..., 2.0034022e-08,\n",
       "         2.1248837e-08, 4.7664871e-05],\n",
       "        [4.8678200e-04, 1.7522238e-07, 4.8286188e-03, ..., 1.8459173e-07,\n",
       "         1.8739529e-07, 4.7664871e-05],\n",
       "        ...,\n",
       "        [9.7570702e-04, 4.4024880e-08, 4.6575265e-03, ..., 4.6430767e-08,\n",
       "         4.8393112e-08, 4.7664871e-05],\n",
       "        [1.7939348e-03, 2.3023816e-08, 1.9682848e-03, ..., 2.4676178e-08,\n",
       "         2.5414703e-08, 4.7664871e-05],\n",
       "        [1.3181325e-02, 2.4368697e-08, 3.8964534e-04, ..., 2.6508491e-08,\n",
       "         2.7652829e-08, 4.7664871e-05]], dtype=float32)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[preprocess_and_predict_sentiment(text) for text in translation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.8466\n",
      "2) neutral 0.1458\n",
      "3) negative 0.0076\n"
     ]
    }
   ],
   "source": [
    "ranking = np.argsort(scores)[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
